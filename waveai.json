{
  "ollama-llama": {
    "display:name": "Ollama - Llama 3.3",
    "display:order": 1,
    "display:icon": "microchip",
    "display:description": "Local Llama 3.3 70B via Ollama (primary general model)",
    "ai:apitype": "openai-chat",
    "ai:model": "llama3.3:70b",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:11434/v1/chat/completions",
    "ai:apitoken": "ollama",
    "ai:capabilities": [
      "tools"
    ]
  },
  "ollama-deepseek-coder": {
    "display:name": "Ollama - DeepSeek Coder V2 16B",
    "display:order": 2,
    "display:icon": "microchip",
    "display:description": "Local DeepSeek coder via Ollama (primary coding model)",
    "ai:apitype": "openai-chat",
    "ai:model": "deepseek-coder-v2:16b",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:11434/v1/chat/completions",
    "ai:apitoken": "ollama",
    "ai:capabilities": [
      "tools"
    ]
  },
  "ollama-deepseek": {
    "display:name": "DeepSeek V2.5 (Primary)",
    "display:order": 1,
    "display:icon": "code",
    "display:description": "236B MoE - coding + reasoning powerhouse",
    "ai:apitype": "openai-chat",
    "ai:model": "deepseek-v2.5",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:11434/v1/chat/completions",
    "ai:apitoken": "ollama",
    "ai:capabilities": ["tools"]
  },
  "ollama-llama-vision": {
    "display:name": "Llama 3.2 Vision 11B",
    "display:order": 3,
    "display:icon": "eye",
    "display:description": "Image understanding, screenshots, diagrams",
    "ai:apitype": "openai-chat",
    "ai:model": "llama3.2-vision:11b",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:11434/v1/chat/completions",
    "ai:apitoken": "ollama",
    "ai:capabilities": ["tools", "images"]
  },
  "lms-qwen": {
    "display:name": "LMS - Qwen 3 Coder Next",
    "display:order": 4,
    "display:icon": "server",
    "display:description": "Local Qwen model via LM Studio (secondary local backend)",
    "ai:apitype": "openai-chat",
    "ai:model": "qwen/qwen3-coder-next",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:1234/v1/chat/completions",
    "ai:apitoken": "not-needed",
    "ai:capabilities": ["tools"]
  },
  "lms-qwen-vision": {
    "display:name": "LMS - Qwen 2.5 VL",
    "display:order": 5,
    "display:icon": "image",
    "display:description": "Advanced vision - documents, OCR, charts",
    "ai:apitype": "openai-chat",
    "ai:model": "qwen/qwen2.5-vl-8b-instruct",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:1234/v1/chat/completions",
    "ai:apitoken": "not-needed",
    "ai:capabilities": ["tools", "images"]
  },
  "lms-glm-vision": {
    "display:name": "LMS - GLM 4.6V Flash",
    "display:order": 6,
    "display:icon": "bolt",
    "display:description": "Fast 9B vision model - already downloading",
    "ai:apitype": "openai-chat",
    "ai:model": "zai-org/glm-4.6v-flash",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:1234/v1/chat/completions",
    "ai:apitoken": "not-needed",
    "ai:capabilities": ["tools", "images"]
  },
  "lms-minimax": {
    "display:name": "LMS - MiniMax M2.1",
    "display:order": 7,
    "display:icon": "server",
    "display:description": "83GB flagship - highest quality local",
    "ai:apitype": "openai-chat",
    "ai:model": "unsloth/MiniMax-M2.1-GGUF-Q2_K",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:1234/v1/chat/completions",
    "ai:apitoken": "not-needed",
    "ai:capabilities": ["tools"]
  },
  "lms-glm-flash": {
    "display:name": "LMS - GLM-4.7 Flash",
    "display:order": 8,
    "display:icon": "bolt",
    "display:description": "24GB fast general model",
    "ai:apitype": "openai-chat",
    "ai:model": "zai-org/glm-4.7-flash",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:1234/v1/chat/completions",
    "ai:apitoken": "not-needed",
    "ai:capabilities": ["tools"]
  },
  "lms-gemma": {
    "display:name": "LMS - Gemma 3 4B",
    "display:order": 9,
    "display:icon": "feather",
    "display:description": "3GB tiny model - ultra fast",
    "ai:apitype": "openai-chat",
    "ai:model": "google/gemma-3-4b",
    "ai:thinkinglevel": "high",
    "ai:endpoint": "http://localhost:1234/v1/chat/completions",
    "ai:apitoken": "not-needed",
    "ai:capabilities": ["tools"]
  },
  "openai-gpt4o-mini": {
    "display:name": "OpenAI - GPT-4o-mini",
    "display:order": 10,
    "display:icon": "cloud",
    "display:description": "Cheap & fast cloud model for overflow",
    "ai:provider": "openai",
    "ai:model": "gpt-4o-mini",
    "ai:capabilities": ["tools", "images", "pdfs"]
  },
  "openai-gpt4o": {
    "display:name": "OpenAI - GPT-4o",
    "display:order": 11,
    "display:icon": "cloud",
    "display:description": "High-quality OpenAI model for hard problems",
    "ai:provider": "openai",
    "ai:model": "gpt-4o",
    "ai:capabilities": ["tools", "images", "pdfs"]
  },
  "openrouter-qwen": {
    "display:name": "OpenRouter - Qwen Coder 32B",
    "display:order": 12,
    "display:icon": "cloud",
    "display:description": "Remote Qwen coder via OpenRouter",
    "ai:provider": "openrouter",
    "ai:model": "qwen/qwen-2.5-coder-32b-instruct",
    "ai:capabilities": [
      "tools", "images"
    ]
  },
  "google-gemini-flash": {
    "display:name": "Gemini 2.0 Flash",
    "display:order": 13,
    "display:icon": "cloud",
    "display:description": "Google Gemini for multimodal tasks",
    "ai:provider": "google",
    "ai:model": "gemini-2.0-flash-exp",
    "ai:capabilities": ["tools", "images", "pdfs", "video", "audio"]
  }
}